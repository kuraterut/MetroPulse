### **Задание на групповой проект: "Аналитическая платформа для сервиса `MetroPulse`"**

**Команда:** до 5 человек
**Продолжительность:** Весь семестр (4 этапа)

#### **Легенда**

Вы — команда Data-инженеров в быстрорастущем стартапе `MetroPulse`. Наш сервис предоставляет пользователям мобильное приложение для отслеживания движения общественного транспорта в реальном времени, планирования поездок и оплаты проезда.

Сервис генерирует два основных типа данных:

1.  **События от транспорта (поток):** Каждые 10 секунд каждый автобус, трамвай и поезд метро отправляет событие с GPS-координатами, скоростью, номером маршрута и ID транспортного средства. Эти данные поступают в топик Kafka `vehicle_positions`.
2.  **Действия пользователей (OLTP):** В основной реляционной базе данных (PostgreSQL) хранятся профили пользователей, история их поездок, транзакции по оплате и построенные маршруты.

**Бизнес-задача:** Руководство компании хочет принимать решения на основе данных. Нам нужна аналитическая платформа, которая позволит:
*   Аналитикам - строить сложные отчеты по историческим данным (например, популярность маршрутов по часам, средняя скорость движения на участках).
*   Менеджерам - видеть оперативные дэшборды с текущей ситуацией в городе (например, количество активных ТС на линии, среднее время ожидания).
*   Data Scientist'ам - иметь доступ к очищенным данным для обучения моделей (например, предсказание времени прибытия).

Ваша задача - спроектировать и реализовать прототип такой платформы с нуля.

---

#### **Этап 1: Проектирование Хранилища Данных (DWH)**

**Цель:** Создать надежный фундамент для всей аналитической платформы — центральное хранилище данных.

**Задачи:**
1.  **Изучить источники:** Проанализировать структуру данных из основной OLTP-базы (вам будет предоставлена схема) и структуру событий из Kafka.
2.  **Выбрать методологию:** Выбрать и **аргументированно обосновать** одну из методологий моделирования DWH:
    *   **Подход Кимбалла (Dimensional Modeling):** Спроектировать схему "звезда" или "снежинка".
    *   **Подход Data Vault 2.0:** Спроектировать хабы, линки и сателлиты.
3.  **Спроектировать DWH:** Разработать детальную логическую и физическую схему для слоев Staging (сырые данные) и Core DWH (очищенные, интегрированные данные).
4.  **Продумать SCD:** Для как минимум одного измерения (например, "Маршрут", у которого может измениться стоимость или схема движения) предусмотреть механизм отслеживания исторических изменений (Slowly Changing Dimensions, Type 2).
5.  **Подготовить DDL:** Написать DDL-скрипты (`CREATE TABLE...`) для всех таблиц вашего DWH. В качестве СУБД для DWH можно использовать PostgreSQL или Greenplum (если есть возможность).

**Результат сдачи:**
*   **Архитектурный документ (10-15 слайдов или 5-7 страниц):**
    *   Описание источников данных.
    *   Обоснование выбора методологии моделирования (Kimball vs. Data Vault).
    *   Логическая модель DWH (ER-диаграмма).
    *   Описание ключевых таблиц (факты, измерения / хабы, линки).
    *   Описание реализации SCD.
*   **Набор DDL-скриптов** в Git-репозитории.
*   **Защита:** Презентация и защита вашей архитектуры.

---

#### **Этап 2: Построение Batch-пайплайна**

**Цель:** Наполнить наше DWH историческими данными, реализовав ETL/ELT-процесс.

**Задачи:**

1.  **Настроить окружение:** Подготовить Docker-окружение, включающее Spark, DWH (PostgreSQL/Greenplum) и эмулятор S3 (MinIO) для хранения "сырых" данных.
2.  **Сгенерировать данные:** Написать скрипты для генерации правдоподобных тестовых данных (пользователи, поездки, транзакции) и выгрузить их в MinIO в формате Parquet.
3.  **Разработать Spark-джобы:** Написать ETL-пайплайн на Apache Spark (PySpark/Scala), который:
    *   Читает сырые данные из MinIO.
    *   Выполняет очистку и трансформации (например, объединяет данные из разных источников, приводит типы, рассчитывает длительность поездки).
    *   Реализует логику загрузки SCD Type 2 для выбранного измерения.
    *   Загружает обработанные данные в целевые таблицы в DWH.
4.  **Оркестрация:** Описать, как бы вы оркестрировали этот пайплайн для ежедневного запуска (например, с помощью Airflow). Реализация не требуется, но схема и описание — большой плюс.

**Результат сдачи:**
*   **Код Spark-джоб** в Git-репозитории.
*   **Docker-compose.yml** и инструкции по запуску всего пайплайна.
*   **Демонстрация:** Показать, как запуск джобы наполняет пустое DWH данными.
*   **Краткий отчет** с описанием логики трансформаций.

---
#### **Этап 3: Построение витрин и Stream-пайплайна**

**Цель:** Предоставить данные конечным пользователям (аналитикам и менеджерам) в удобном и быстром виде. **Команда должна выбрать и реализовать ОДИН из двух вариантов.**

##### **Вариант А: Batch-витрина для BI-аналитики**

1.  **Задача:** Создать агрегированные витрины данных (Data Marts) для быстрой отчетности. Например, витрина с агрегатами по поездкам (количество, средняя стоимость, средняя длительность) по каждому маршруту за каждый час.
2.  **Технологии:**
    *   Написать Spark-джобу, которая читает данные из Core DWH.
    *   Агрегирует их.
    *   Загружает результат в быструю аналитическую СУБД **ClickHouse**.
3.  **Результат:**
    *   Код Spark-джобы для построения витрин.
    *   DDL для таблиц в ClickHouse.
    *   Демонстрация работы: показать, как сложные аналитические запросы к ClickHouse выполняются на порядки быстрее, чем к основному DWH.

##### **Вариант Б: Real-time витрина для оперативного мониторинга**

1.  **Задача:** Создать дэшборд, который показывает ситуацию в городе "в прямом эфире". Например, количество активных ТС на каждом маршруте с обновлением каждые 30 секунд.
2.  **Технологии:**
    *   Настроить генератор событий, который пишет данные о движении транспорта в **Kafka**.
    *   Написать джобу на **Apache Flink**, которая читает данные из Kafka.
    *   Выполняет оконные агрегации (например, `TumblingWindow` на 30 секунд).
    *   Записывает результат в быструю In-Memory СУБД (**Redis** или **Picodata/Tarantool**).
3.  **Результат:**
    *   Код Flink-джобы.
    *   Настроенный end-to-end пайплайн (генератор -> Kafka -> Flink -> Redis).
    *   Демонстрация работы: показать, как данные в Redis обновляются в реальном времени.

---

#### **Этап 4: Финальная защита**

**Цель:** Презентовать всю проделанную работу как единый, целостный проект.

**Задачи:**

1.  **Подготовить презентацию (15 минут):**
    *   Кратко напомнить бизнес-задачу.
    *   Показать финальную архитектуру всей платформы.
    *   Обосновать ключевые технологические и архитектурные решения (Kimball vs DV, Spark, выбор для Этапа 3).
    *   Продемонстрировать работу ключевых частей пайплайна.
    *   Рассказать о возникших трудностях и как вы их решали.
    *   Описать возможные пути развития платформы.
2.  **Ответить на вопросы** преподавателя и других команд.

**Критерии финальной оценки:**
*   **Техническая реализация:** Корректность, чистота кода, работоспособность решения.
*   **Архитектурная проработка:** Обоснованность и глубина принятых решений.
*   **Полнота решения:** Все этапы выполнены и связаны в единую систему.
*   **Качество презентации и защиты:** Умение четко и ясно донести свои идеи.
*   **Командная работа:** Равномерный вклад всех участников.
По каждому критерию выставляется оценка от 0 до 10.
Финальная оценка - среднее арифметическое от всех оценок по критериям.
